{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved CNN for FT of MNIST data. Also padded size.\n",
    "\n",
    "### v3 introduces 2 channels of intensity and phase to see if accuracy improves\n",
    "\n",
    "### v4 introduces hyperas to do hyper-param optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "apJbCsBHl-2A"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Choose GPUs and CPUs\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "\n",
    "#If want to choose GPU to use\n",
    "#This will make it so that only the 0th GPU is visible to TensorFlow.\n",
    "#In your case, you can choose any in the range [0, 3].\n",
    "#If you wanted, for example, the 0th and 2nd GPUs to both be visible, replace \"0\" with \"0,2\"\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "GPU=True\n",
    "CPU=False\n",
    "num_cores = 8\n",
    "\n",
    "if GPU:\n",
    "    num_GPU = 2\n",
    "    num_CPU = 2\n",
    "if CPU:\n",
    "    num_CPU = num_cores\n",
    "    num_GPU = 0\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU},\\\n",
    "#        gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.5))\n",
    "                       )\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.utils import to_categorical, multi_gpu_model\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "from scipy.fftpack import fftn,fftshift\n",
    "\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice,uniform\n",
    "\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST data from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 28, 28\n",
    "    num_classes=10\n",
    "\n",
    "    # the data, split between train and test sets\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "    train_dataset = x_train.astype('float32')\n",
    "    test_dataset = x_test.astype('float32')\n",
    "    train_dataset /= 255\n",
    "    test_dataset /= 255\n",
    "    print('train_dataset shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    train_labels = keras.utils.to_categorical(y_train, num_classes)\n",
    "    test_labels = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "    # Pad numpy array\n",
    "    # Want 36x36 which is how big the multiMNIST will be\n",
    "    train_dataset=np.pad(train_dataset, ((0,0),(4,4),(4,4),(0,0)), 'constant', constant_values=(0,0))\n",
    "    print (train_dataset.shape)\n",
    "\n",
    "    test_dataset=np.pad(test_dataset, ((0,0),(4,4),(4,4),(0,0)), 'constant', constant_values=(0,0))\n",
    "    print (test_dataset.shape)\n",
    "\n",
    "    # Take FT\n",
    "    dims=train_dataset.shape\n",
    "    ft_train=np.zeros((dims[0], 36, 36, 2), float)\n",
    "    for i in range(dims[0]):\n",
    "        ft_train[i,:,:,0]=np.abs(fftshift(fftn(train_dataset[i,:,:]))).squeeze()\n",
    "        ft_train[i,:,:,1]=np.angle(fftshift(fftn(train_dataset[i,:,:]))).squeeze()\n",
    "    #    ft_train[i,:,:,1]=unwrap_phase(ft_train[i,:,:,1])\n",
    "\n",
    "    dims=test_dataset.shape\n",
    "    ft_test=np.zeros((dims[0], 36, 36, 2), float)\n",
    "    for i in tqdm_notebook(range(dims[0])):\n",
    "        ft_test[i,:,:,0]=np.abs(fftshift(fftn(test_dataset[i,:,:]))).squeeze()\n",
    "        ft_test[i,:,:,1]=np.angle(fftshift(fftn(test_dataset[i,:,:]))).squeeze()\n",
    "    #    ft_test[i,:,:,1]=unwrap_phase(ft_test[i,:,:,1])\n",
    "\n",
    "\n",
    "    dims=ft_train.shape\n",
    "    print (dims)\n",
    "    trainee=ft_train.reshape(dims[0],dims[1],dims[2],2)\n",
    "    dims=test_dataset.shape\n",
    "    testee=ft_test.reshape(dims[0],dims[1],dims[2],2)\n",
    "    print (trainee.shape)\n",
    "    return trainee, train_labels, testee, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O.K now try CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass ModelMGPU(Model):\\n    def __init__(self, ser_model, gpus):\\n        pmodel = multi_gpu_model(ser_model, gpus)\\n        self.__dict__.update(pmodel.__dict__)\\n        self._smodel = ser_model\\n\\n    def __getattribute__(self, attrname):\\n        '''Override load and save methods to be used from the serial-model. The\\n        serial-model holds references to the weights in the multi-gpu model.\\n        '''\\n        # return Model.__getattribute__(self, attrname)\\n        if 'load' in attrname or 'save' in attrname:\\n            return getattr(self._smodel, attrname)\\n\\n        return super(ModelMGPU, self).__getattribute__(attrname)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class ModelMGPU(Model):\n",
    "    def __init__(self, ser_model, gpus):\n",
    "        pmodel = multi_gpu_model(ser_model, gpus)\n",
    "        self.__dict__.update(pmodel.__dict__)\n",
    "        self._smodel = ser_model\n",
    "\n",
    "    def __getattribute__(self, attrname):\n",
    "        '''Override load and save methods to be used from the serial-model. The\n",
    "        serial-model holds references to the weights in the multi-gpu model.\n",
    "        '''\n",
    "        # return Model.__getattribute__(self, attrname)\n",
    "        if 'load' in attrname or 'save' in attrname:\n",
    "            return getattr(self._smodel, attrname)\n",
    "\n",
    "        return super(ModelMGPU, self).__getattribute__(attrname)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Conv2D, MaxPool2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential, Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Dropout, Activation, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import to_categorical, multi_gpu_model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tqdm import tqdm_notebook\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from scipy.fftpack import fftn, fftshift\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'w_size': hp.choice('w_size', [2,3,4]),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: # input image dimensions\n",
      "   3: img_rows, img_cols = 28, 28\n",
      "   4: num_classes=10\n",
      "   5: \n",
      "   6: # the data, split between train and test sets\n",
      "   7: (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
      "   8: \n",
      "   9: if K.image_data_format() == 'channels_first':\n",
      "  10:     x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
      "  11:     x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
      "  12:     input_shape = (1, img_rows, img_cols)\n",
      "  13: else:\n",
      "  14:     x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
      "  15:     x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
      "  16:     input_shape = (img_rows, img_cols, 1)\n",
      "  17: \n",
      "  18: train_dataset = x_train.astype('float32')\n",
      "  19: test_dataset = x_test.astype('float32')\n",
      "  20: train_dataset /= 255\n",
      "  21: test_dataset /= 255\n",
      "  22: print('train_dataset shape:', x_train.shape)\n",
      "  23: print(x_train.shape[0], 'train samples')\n",
      "  24: print(x_test.shape[0], 'test samples')\n",
      "  25: \n",
      "  26: # convert class vectors to binary class matrices\n",
      "  27: train_labels = keras.utils.to_categorical(y_train, num_classes)\n",
      "  28: test_labels = keras.utils.to_categorical(y_test, num_classes)\n",
      "  29: \n",
      "  30: \n",
      "  31: # Pad numpy array\n",
      "  32: # Want 36x36 which is how big the multiMNIST will be\n",
      "  33: train_dataset=np.pad(train_dataset, ((0,0),(4,4),(4,4),(0,0)), 'constant', constant_values=(0,0))\n",
      "  34: print (train_dataset.shape)\n",
      "  35: \n",
      "  36: test_dataset=np.pad(test_dataset, ((0,0),(4,4),(4,4),(0,0)), 'constant', constant_values=(0,0))\n",
      "  37: print (test_dataset.shape)\n",
      "  38: \n",
      "  39: # Take FT\n",
      "  40: dims=train_dataset.shape\n",
      "  41: ft_train=np.zeros((dims[0], 36, 36, 2), float)\n",
      "  42: for i in range(dims[0]):\n",
      "  43:     ft_train[i,:,:,0]=np.abs(fftshift(fftn(train_dataset[i,:,:]))).squeeze()\n",
      "  44:     ft_train[i,:,:,1]=np.angle(fftshift(fftn(train_dataset[i,:,:]))).squeeze()\n",
      "  45: #    ft_train[i,:,:,1]=unwrap_phase(ft_train[i,:,:,1])\n",
      "  46: \n",
      "  47: dims=test_dataset.shape\n",
      "  48: ft_test=np.zeros((dims[0], 36, 36, 2), float)\n",
      "  49: for i in tqdm_notebook(range(dims[0])):\n",
      "  50:     ft_test[i,:,:,0]=np.abs(fftshift(fftn(test_dataset[i,:,:]))).squeeze()\n",
      "  51:     ft_test[i,:,:,1]=np.angle(fftshift(fftn(test_dataset[i,:,:]))).squeeze()\n",
      "  52: #    ft_test[i,:,:,1]=unwrap_phase(ft_test[i,:,:,1])\n",
      "  53: \n",
      "  54: \n",
      "  55: dims=ft_train.shape\n",
      "  56: print (dims)\n",
      "  57: trainee=ft_train.reshape(dims[0],dims[1],dims[2],2)\n",
      "  58: dims=test_dataset.shape\n",
      "  59: testee=ft_test.reshape(dims[0],dims[1],dims[2],2)\n",
      "  60: print (trainee.shape)\n",
      "  61: \n",
      "  62: \n",
      "  63: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3: \n",
      "   4:     w_size=space['w_size']\n",
      "   5: \n",
      "   6:     model=Sequential()\n",
      "   7: \n",
      "   8:     model.add(Conv2D(32,(w_size,w_size), activation='relu', input_shape=(36,36,2),\n",
      "   9:                      data_format='channels_last', padding='same'))\n",
      "  10:     model.add(Conv2D(32,(w_size,w_size), activation='relu', padding='same'))\n",
      "  11:     model.add(MaxPool2D(pool_size=(2,2)))\n",
      "  12:     model.add(Conv2D(64,(w_size,w_size), activation='relu', padding='same'))\n",
      "  13:     model.add(Conv2D(64,(w_size,w_size), activation='relu', padding='same'))\n",
      "  14:     model.add(MaxPool2D(pool_size=(2,2)))\n",
      "  15:     model.add(Conv2D(128,(w_size,w_size), activation='relu', padding='same'))\n",
      "  16:     model.add(Conv2D(128,(w_size,w_size), activation='relu', padding='same'))\n",
      "  17:     model.add(MaxPool2D(pool_size=(2,2)))\n",
      "  18:     model.add(Flatten())\n",
      "  19:     model.add(Dense(600, activation='relu'))\n",
      "  20:     model.add(Dropout(space['Dropout']))\n",
      "  21:     model.add(Dense(10, activation='sigmoid')) \n",
      "  22: \n",
      "  23:     #parallel_model = ModelMGPU(model, gpus=num_GPU)\n",
      "  24:     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "  25:     \n",
      "  26:     history=model.fit(trainee, train_labels, batch_size=256, verbose=0, epochs=10, \n",
      "  27:                   validation_split = 0.1)#, callbacks=[checkpoints])\n",
      "  28:     \n",
      "  29:     score=model.evaluate(testee, test_labels)\n",
      "  30:     \n",
      "  31:     #get the highest validation accuracy of the training epochs\n",
      "  32:     validation_acc = np.amax(history.history['val_acc'])\n",
      "  33:     print(validation_acc)\n",
      "  34:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
      "  35: \n",
      "train_dataset shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "(60000, 36, 36, 1)\n",
      "(10000, 36, 36, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcherukara/miniconda3/envs/gpu/lib/python3.6/site-packages/scipy/fftpack/basic.py:160: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  z[index] = x\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e0916fdd90417abf8c82417e4b50ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(60000, 36, 36, 2)\n",
      "(60000, 36, 36, 2)\n",
      "10000/10000 [==============================] - 1s 67us/step\n",
      "0.9823333328564962\n",
      "10000/10000 [==============================] - 0s 47us/step\n",
      "0.9814999995231628\n",
      "10000/10000 [==============================] - 1s 52us/step\n",
      "0.9801666668256124\n",
      "10000/10000 [==============================] - 0s 47us/step\n",
      "0.9813333328564962\n",
      "10000/10000 [==============================] - 0s 46us/step\n",
      "0.9829999995231629\n",
      "10000/10000 [==============================] - 1s 67us/step\n",
      "0.9834999998410543\n",
      "10000/10000 [==============================] - 0s 47us/step\n",
      "0.9835000001589457\n",
      "10000/10000 [==============================] - 1s 68us/step\n",
      "0.9835000001589457\n",
      "10000/10000 [==============================] - 0s 50us/step\n",
      "0.9833333328564962\n",
      "10000/10000 [==============================] - 1s 54us/step\n",
      "0.9843333328564962\n"
     ]
    }
   ],
   "source": [
    "def create_model(trainee, train_labels, testee, test_labels):\n",
    "\n",
    "    w_size={{choice([2,3,4])}}\n",
    "\n",
    "    model=Sequential()\n",
    "\n",
    "    model.add(Conv2D(32,(w_size,w_size), activation='relu', input_shape=(36,36,2),\n",
    "                     data_format='channels_last', padding='same'))\n",
    "    model.add(Conv2D(32,(w_size,w_size), activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(64,(w_size,w_size), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(64,(w_size,w_size), activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(128,(w_size,w_size), activation='relu', padding='same'))\n",
    "    model.add(Conv2D(128,(w_size,w_size), activation='relu', padding='same'))\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(600, activation='relu'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense(10, activation='sigmoid')) \n",
    "\n",
    "    #parallel_model = ModelMGPU(model, gpus=num_GPU)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    history=model.fit(trainee, train_labels, batch_size=256, verbose=0, epochs=10, \n",
    "                  validation_split = 0.1)#, callbacks=[checkpoints])\n",
    "    \n",
    "    score=model.evaluate(testee, test_labels)\n",
    "    \n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(history.history['val_acc'])\n",
    "    print(validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
    "    \n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                          data=data,\n",
    "                          algo=tpe.suggest,\n",
    "                          max_evals=10,\n",
    "                          trials=Trials(),\n",
    "                          notebook_name='v4_hyperas_optimized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "(60000, 36, 36, 1)\n",
      "(10000, 36, 36, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f651164c140542acaea653be0fcae47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(60000, 36, 36, 2)\n",
      "(60000, 36, 36, 2)\n"
     ]
    }
   ],
   "source": [
    "trainee, train_labels, testee, test_labels=data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 55us/step\n",
      "[0.07110347990648005, 0.9816]\n"
     ]
    }
   ],
   "source": [
    "#from keras.models import Model, load_model\n",
    "#model=load_model('ft_models/weights.06.hdf5') #Numbering starts from 0\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "score=best_model.evaluate(testee, test_labels, verbose=1)\n",
    "print (score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dropout': 0.22780041552649732, 'w_size': 1}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_55 (Conv2D)           (None, 36, 36, 32)        608       \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 36, 36, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 18, 18, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 18, 18, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 18, 18, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 9, 9, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 9, 9, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 9, 9, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 600)               1229400   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                6010      \n",
      "=================================================================\n",
      "Total params: 1,522,130\n",
      "Trainable params: 1,522,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (best_run)\n",
    "print (best_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=(model.predict(testee))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=100\n",
    "f,axarr=plt.subplots(1,2,figsize=(8, 6))\n",
    "axarr[0].imshow(test_dataset[i].squeeze(),cmap='viridis')\n",
    "img=axarr[1].imshow(testee[i,:,:,1].squeeze(), cmap='viridis')\n",
    "plt.colorbar(img, fraction=0.046, pad=0.04)\n",
    "print (np.argmax(preds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "1_notmnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
